# lora_config.yaml
model:        ./mlx_smallLM               # folder with weights.npz + config.json
tokenizer:    tokenizer/spm.model         # same tokenizer used during pre-training

train:        true
data:         data/qa_prompt_completion/train.jsonl
val:          data/qa_prompt_completion/valid.jsonl

train_type:   lora
rank:         16
batch_size:   16
learning_rate: 5e-4
epochs:       3
mask_prompt:  true
use_chat_template: false

out:          adapters/smallLM_qa.npz
save_every:   1000

# These  GPT-2-style fields must *match* the built-in GPT-2 class
model_type:   gpt2
n_ctx:        512
n_positions:  512
n_embd:       768
n_head:       12
n_layer:      18
layer_norm_epsilon: 1.0e-6
vocab_size:   50096