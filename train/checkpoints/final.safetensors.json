{
  "args": {
    "add_bos": true,
    "add_eos": true,
    "attention_impl": "fast",
    "backend": "ring",
    "batch_size": 1,
    "collective_stream": "default",
    "config": "",
    "d_model": 256,
    "data_mode": "tokens",
    "dtype": "float32",
    "eval_every": 0,
    "eval_steps": 20,
    "expected_world": 2,
    "grad_accum": 1,
    "grad_clip": 0.5,
    "ignore_index": -100,
    "log_every": 1,
    "lr": 2e-05,
    "max_seq_len": 64,
    "max_steps": 5,
    "min_lr_ratio": 0.1,
    "mlp_ratio": 4.0,
    "n_heads": 4,
    "n_layers": 3,
    "optimizer_rank0_only": false,
    "resume": "",
    "save_dir": "/Users/williamzebrowski/sml-mlx/train/checkpoints",
    "save_every": 0,
    "seed": 1337,
    "spm_model": "/Users/williamzebrowski/sml-mlx/tokenizer/fineweb_spm/spm.model",
    "token_dtype": "uint16",
    "trace_first_step": true,
    "train_sources": "",
    "train_tokens": "/Users/williamzebrowski/sml-mlx/train/data/debug_tokens.npy",
    "val_sources": "",
    "val_tokens": "",
    "vocab_size": 30000,
    "warmup_steps": 200,
    "weight_decay": 0.0
  },
  "backend": "ring",
  "config": {
    "attention_impl": "fast",
    "bias": false,
    "d_model": 256,
    "max_seq_len": 64,
    "mlp_ratio": 4.0,
    "n_heads": 4,
    "n_layers": 3,
    "rope_base": 10000.0,
    "vocab_size": 30000
  },
  "duration_sec": 0.5582035409897799,
  "step": 5,
  "timestamp": 1771808862.10862,
  "world": 2
}