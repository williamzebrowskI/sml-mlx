{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfd015d",
   "metadata": {},
   "source": [
    "# MLX‑Test: Pedagogical Walk‑Through of Your OpenELM Architecture\n",
    "\n",
    "This notebook is meant to **teach** every moving part of the exact model you are training.  \n",
    "For each major block you’ll find:\n",
    "\n",
    "1. **Purpose & intuition** – why the block exists and design trade‑offs.  \n",
    "2. **Key equations / pseudocode** in plain English.  \n",
    "3. **The real MLX implementation** from your repo so you can run or edit live.\n",
    "\n",
    "Feel free to experiment: change hidden sizes, activation functions, etc. and re‑run cells.\n",
    "\n",
    "*Generated 2025‑07‑14 02:38*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e54f3b",
   "metadata": {},
   "source": [
    "## 1. Hyper‑Parameter Dataclass — `SMLMConfig`\n",
    "\n",
    "All hyper‑parameters (HPs) are centralised in a dataclass so the rest of the model\n",
    "can stay clean.  We separate:\n",
    "\n",
    "* **Architecture HPs** (e.g. `model_dim`, `head_dim`, number of layers).\n",
    "* **Training HPs** (batch size, learning‑rate schedule, etc.).\n",
    "\n",
    "Loading from *config.json* keeps experiments reproducible and makes sweeps trivial\n",
    "— swap JSON files, not code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses, json, pathlib\n",
    "from typing import List\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SMLMConfig:\n",
    "    \"\"\"Model + training hyper‑parameters.\"\"\"\n",
    "    tokenizer_path: str\n",
    "    checkpoint_dir: str\n",
    "    vocab_size: int\n",
    "    model_dim: int\n",
    "    num_transformer_layers: int\n",
    "    head_dim: int\n",
    "    num_query_heads: List[int]\n",
    "    num_kv_heads: List[int]\n",
    "    num_gqa_groups: int\n",
    "    normalize_qk_projections: bool\n",
    "    ffn_multipliers: List[float]\n",
    "    ffn_dim_divisor: int\n",
    "    ffn_with_glu: bool\n",
    "    rope_freq_constant: int\n",
    "    rope_max_length: int\n",
    "    normalization_layer_name: str\n",
    "    activation_fn_name: str\n",
    "    initializer_range: float\n",
    "    share_input_output_layers: bool\n",
    "    # training‑time HPs omitted for brevity …\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path):\n",
    "        return cls(**json.loads(pathlib.Path(path).read_text()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceef5c6",
   "metadata": {},
   "source": [
    "## 2. Utility Helpers\n",
    "\n",
    "* **`RMSNorm`** – root‑mean‑square layer‑norm: scale‑invariant and cheaper than LN.  \n",
    "* **`repeat_kv`** – duplicates keys/values so multiple *query* heads can share them\n",
    "  (Grouped‑Query Attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn, mlx.core as mx\n",
    "\n",
    "RMSNorm = nn.RMSNorm  # thin alias\n",
    "\n",
    "def repeat_kv(x: mx.array, n: int, axis: int = 2):\n",
    "    \"\"\"Tile along `axis` (usually head dim) for grouped‑query attention.\"\"\"\n",
    "    return mx.repeat(x, n, axis=axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290194e",
   "metadata": {},
   "source": [
    "## 3. Position‑wise Feed‑Forward Network (FFN)\n",
    "\n",
    "### Why?\n",
    "Each token, **independently**, gets a non‑linear transformation after attention.\n",
    "It mixes information across hidden features (but not across sequence length).\n",
    "\n",
    "### Design choices\n",
    "| Choice | Your value | Reason |\n",
    "|---|---|---|\n",
    "| Hidden multiplier schedule | `ffn_multipliers` array | Saves params in early layers, more capacity deeper |\n",
    "| GLU variant | **SwiGLU** when `ffn_with_glu=True` | Improves expressiveness with a tiny cost |\n",
    "| Activation | SiLU (aka Swish) | Smooth & self‑gating, often better than ReLU/GELU |\n",
    "\n",
    "Equation (with GLU):  \n",
    "\\[\\text{FFN}(x)=W_{2}( \\sigma(W_{1,a}x)\\odot W_{1,b}x )\\]\n",
    "\n",
    "Below is the exact implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, mlx.nn as nn, mlx.core as mx\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg: SMLMConfig, idx: int):\n",
    "        super().__init__()\n",
    "        mult = cfg.ffn_multipliers[idx]\n",
    "        hidden = math.ceil(mult * cfg.model_dim / cfg.ffn_dim_divisor) * cfg.ffn_dim_divisor\n",
    "        out_feats = hidden * 2 if cfg.ffn_with_glu else hidden\n",
    "\n",
    "        # 3.1 Projection to hidden (or 2× hidden for GLU)\n",
    "        self.proj_in = nn.Linear(cfg.model_dim, out_feats, bias=False)\n",
    "\n",
    "        # 3.2 Non‑linearity\n",
    "        self.act = nn.SiLU() if cfg.activation_fn_name == \"swish\" else nn.GELU()\n",
    "        self.use_glu = cfg.ffn_with_glu\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "        # 3.3 Back to model_dim\n",
    "        self.proj_out = nn.Linear(hidden, cfg.model_dim, bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = self.proj_in(x)\n",
    "        if self.use_glu:\n",
    "            a, b = mx.split(y, 2, axis=-1)  # gate & value\n",
    "            y = self.act(a) * b\n",
    "        else:\n",
    "            y = self.act(y)\n",
    "        return self.proj_out(self.dropout(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d1d7c",
   "metadata": {},
   "source": [
    "## 4. Grouped‑Query Attention (GQA) with Rotary Positional Encoding\n",
    "\n",
    "### 4.1 Quick refresher  \n",
    "Multi‑Head Attention lets each token attend to previous tokens.  \n",
    "GQA reduces **KV** redundancy: many *query* heads share a smaller set of *key/value* heads.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.2 Head shapes\n",
    "\n",
    "| Symbol | Value in layer *i* |\n",
    "|---|---|\n",
    "| \\(H_q\\) query heads | `num_query_heads[i]` |\n",
    "| \\(H_{kv}\\) key/value heads | `num_kv_heads[i]` |\n",
    "| groups (=\\(H_q/H_{kv}\\)) | `num_gqa_groups` |\n",
    "\n",
    "### 4.3 Positional Encoding  \n",
    "**RoPE** rotates Q & K in complex plane so attention remains length‑agnostic.\n",
    "\n",
    "### 4.4 Implementation steps\n",
    "1. Linear projection → \\([Q;K;V]\\)  \n",
    "2. Apply RoPE to Q & K  \n",
    "3. `repeat_kv` to share K,V across query groups  \n",
    "4. `mx.fast.scaled_dot_product_attention` (Metal accelerated)  \n",
    "5. Output projection back to `model_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQAttention(nn.Module):\n",
    "    def __init__(self, cfg: SMLMConfig, idx: int):\n",
    "        super().__init__()\n",
    "        Hq, Hkv, D = cfg.num_query_heads[idx], cfg.num_kv_heads[idx], cfg.head_dim\n",
    "        self.Hq, self.Hkv, self.D = Hq, Hkv, D\n",
    "        self.groups = cfg.num_gqa_groups\n",
    "\n",
    "        # total projection size = (Hq + 2*Hkv) * D\n",
    "        self.qkv = nn.Linear(cfg.model_dim, (Hq + 2*Hkv) * D, bias=False)\n",
    "        self.rope = nn.RoPE(D, base=cfg.rope_freq_constant)\n",
    "        self.proj_out = nn.Linear(Hq * D, cfg.model_dim, bias=False)\n",
    "\n",
    "    def __call__(self, x, *, mask):\n",
    "        B, L, _ = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, L, self.Hq + 2*self.Hkv, self.D).transpose(0,2,1,3)\n",
    "        q, k, v = mx.split(qkv, [self.Hq, self.Hq+self.Hkv], axis=1)\n",
    "\n",
    "        q, k = self.rope(q), self.rope(k)               # positional info\n",
    "        k = repeat_kv(k, self.groups, axis=1)\n",
    "        v = repeat_kv(v, self.groups, axis=1)\n",
    "\n",
    "        attn = mx.fast.scaled_dot_product_attention(\n",
    "            q, k, v, scale=1/math.sqrt(self.D), mask=mask)\n",
    "\n",
    "        out = attn.transpose(0,2,1,3).reshape(B, L, -1)\n",
    "        return self.proj_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09205eda",
   "metadata": {},
   "source": [
    "## 5. Decoder Layer (Pre‑Norm Residual)\n",
    "\n",
    "Sequence of operations:\n",
    "\n",
    "1. **RMSNorm** on input  \n",
    "2. **Attention** + residual add  \n",
    "3. **RMSNorm**  \n",
    "4. **Feed‑Forward** + residual add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af36bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, cfg: SMLMConfig, idx):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(cfg.model_dim, eps=1e-6)\n",
    "        self.attn  = GQAttention(cfg, idx)\n",
    "        self.norm2 = RMSNorm(cfg.model_dim, eps=1e-6)\n",
    "        self.ffn   = FeedForward(cfg, idx)\n",
    "    def __call__(self, x, *, mask):\n",
    "        x = x + self.attn(self.norm1(x), mask=mask)\n",
    "        return x + self.ffn(self.norm2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1e46d",
   "metadata": {},
   "source": [
    "## 6. Full Decoder\n",
    "\n",
    "* **Embedding layer** – projects tokens to `model_dim`.  \n",
    "* **Stack of decoder layers** – deep computation.  \n",
    "* **Final RMSNorm** – stabilises outputs.  \n",
    "* **LM head** – linear layer to vocab.  Weight‑tying with embedding reduces params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenELM(nn.Module):\n",
    "    def __init__(self, cfg: SMLMConfig):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.vocab_size, cfg.model_dim)\n",
    "        self.layers = [DecoderLayer(cfg, i) for i in range(cfg.num_transformer_layers)]\n",
    "        self.final_norm = RMSNorm(cfg.model_dim, eps=1e-6)\n",
    "        self.lm_head = nn.Linear(cfg.model_dim, cfg.vocab_size, bias=False)\n",
    "        if cfg.share_input_output_layers:\n",
    "            self.lm_head.weight = self.emb.weight\n",
    "    def __call__(self, tokens):\n",
    "        B, L = tokens.shape\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(L)\n",
    "        h = self.emb(tokens)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, mask=mask)\n",
    "        return self.lm_head(self.final_norm(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
